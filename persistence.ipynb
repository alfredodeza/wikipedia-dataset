{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence and Efficency\n",
    "You now might be aware of some of the problems with parsing several pages and how that can quicly get out of hand. While parsing, you might chose to persist the collected data, so that it can be analyzed and cleaned later. \n",
    "Parsing, retrieving, saving, and cleaning data are all separate actions, and you shouldn't try to work with data while collecting. In this notebook you'll practice further parsing techniques along with persistency, both for JSON and CSV formats as well as using SQL and a database.\n",
    "\n",
    "Start by loading one of the available HTML files into the `scrapy` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "current_dir = os.path.abspath('')\n",
    "url = os.path.join(current_dir, \"html/1992_World_Junior_Championships_in_Athletics_–_Men's_high_jump\")\n",
    "with open(url) as _f:\n",
    "    url_data = _f.read()\n",
    "\n",
    "response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Jagan Hames 2.23\n",
      "2nd Antoine Burke 2.20\n",
      "3rd Mika Polku 2.20\n",
      "4 Attila Zsivoczky 2.20\n",
      "5 Roland Stark 2.15\n",
      "6 Oskari Frösén 2.15\n",
      "7 Shunichi Kobayashi 2.10\n",
      "7 Stefan Holm 2.10\n",
      "9 Tomohiro Nomura 2.10\n",
      "10 Henry Patterson 2.10\n",
      "11 Bjørn Olsson 2.10\n",
      "12 Stuart Ohrland 2.05\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the interesting data is available. Interesting data means we have finalists and all the other athletes.\n",
    "def find_finalists_table(response):\n",
    "    for table in response.xpath('//table'):\n",
    "        body = table.xpath('tbody')\n",
    "        # check if len of body is more than 12, otherwise skip it\n",
    "        if len(body.xpath('tr')) < 12:\n",
    "            continue\n",
    "        # now check if we have medalists\n",
    "        for tr in body.xpath('tr'):\n",
    "            if tr.xpath('td/span/img/@alt').extract_first():\n",
    "                return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(response):\n",
    "    table = find_finalists_table(response)\n",
    "    results = []\n",
    "    # extract the year from the title\n",
    "    year = response.xpath('//title/text()').extract_first().split()[0]\n",
    "\n",
    "    for tr in table.xpath('tbody').xpath('tr'):\n",
    "        # extract the text value of alt attribute of the img tag\n",
    "        medal = tr.xpath('td/span/img/@alt').extract_first()\n",
    "        if medal and \"medalist\" in medal:\n",
    "            place = medal.split()[0]\n",
    "        else:\n",
    "            try:\n",
    "                place, _ = tr.xpath('td/text()').extract()\n",
    "            except ValueError:\n",
    "                pass\n",
    "        try:\n",
    "            athlete = tr.xpath('td/a/text()').extract_first()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "        if not athlete and not medal:\n",
    "            continue\n",
    "        height = tr.xpath('td/b/text()').extract_first()\n",
    "        #print(place, athlete, height, year)\n",
    "        results.append([place, athlete, height, year])\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['1st', 'Alfredo Deza', '2.21', '1998'],\n",
       "  ['2nd', 'Yin Xueli', '2.21', '1998'],\n",
       "  ['3rd', 'Aleksandr Veryutin', '2.21', '1998'],\n",
       "  ['4', 'Mike Ponikvar', '2.21', '1998'],\n",
       "  ['5', 'Yaroslav Rybakov', '2.18', '1998'],\n",
       "  ['6', 'Pawel Mankiewicz', '2.15', '1998'],\n",
       "  ['7', 'Naoyuki Daigo', '2.15', '1998'],\n",
       "  ['8', 'Ronald Garlett', '2.10', '1998'],\n",
       "  ['8', 'Martin Lloyd', '2.10', '1998'],\n",
       "  ['8', 'Casper Labuschagne', '2.10', '1998'],\n",
       "  ['8', 'Nikolay Korostelyov', '2.10', '1998'],\n",
       "  ['12', 'Einar Karl Hjartarson', '2.10', '1998'],\n",
       "  ['12', 'Vusumzi Mtshatseni', '2.10', '1998'],\n",
       "  ['14', 'James Carr', '2.10', '1998']],\n",
       " [['1st', 'Mark Boswell', '2.24', '1996'],\n",
       "  ['2nd', 'Svatoslav Ton', '2.21', '1996'],\n",
       "  ['2nd', 'Ben Challenger', '2.21', '1996'],\n",
       "  ['4', 'Dave Furman', '2.18', '1996'],\n",
       "  ['5', 'Toni Huikuri', '2.18', '1996'],\n",
       "  ['6', 'Dejan Vreljakovic', '2.18', '1996'],\n",
       "  ['7', 'James Brierley', '2.15', '1996'],\n",
       "  ['7', 'Roman Fricke', '2.15', '1996'],\n",
       "  ['9', 'François Potgieter', '2.15', '1996'],\n",
       "  ['10', 'Yannick Tregaro', '2.10', '1996'],\n",
       "  ['11', 'Tivadar Kovács', '2.10', '1996'],\n",
       "  ['12', 'Noriyasu Arai', '2.05', '1996']],\n",
       " [['1st', 'Steve Smith', '2.37', '1992'],\n",
       "  ['2nd', 'Tim Forsyth', '2.31', '1992'],\n",
       "  ['3rd', 'Takahiro Kimino', '2.29', '1992'],\n",
       "  ['4', 'Kim Tae Young', '2.23', '1992'],\n",
       "  ['5', 'Sergey Klyugin', '2.20', '1992'],\n",
       "  ['5', 'Kristofer Lamos', '2.20', '1992'],\n",
       "  ['7', 'Tomáš Janků', '2.17', '1992'],\n",
       "  ['8', 'Coenraad Roux', '2.17', '1992'],\n",
       "  ['9', 'Clayton Pugh', '2.17', '1992'],\n",
       "  ['10', 'Xu Xiaodong', '2.14', '1992'],\n",
       "  ['10', 'Sven Ootjers', '2.14', '1992'],\n",
       "  ['12', 'Mirko Zanotti', '2.14', '1992'],\n",
       "  ['13', 'Clifford van Reed', '2.14', '1992'],\n",
       "  ['14', 'Dejan Miloševic', '2.10', '1992'],\n",
       "  ['15', 'Hugo Muñoz', '2.10', '1992'],\n",
       "  ['16', 'Stanley Osuide', '2.05', '1992'],\n",
       "  ['16', 'Kostas Liapis', '2.05', '1992'],\n",
       "  ['16', 'Kim Tae-hoi', '2.05', '1992'],\n",
       "  ['19', 'Antoine Burke', '2.00', '1992']],\n",
       " [['1st', 'Jagan Hames', '2.23', '1994'],\n",
       "  ['2nd', 'Antoine Burke', '2.20', '1994'],\n",
       "  ['3rd', 'Mika Polku', '2.20', '1994'],\n",
       "  ['4', 'Attila Zsivoczky', '2.20', '1994'],\n",
       "  ['5', 'Roland Stark', '2.15', '1994'],\n",
       "  ['6', 'Oskari Frösén', '2.15', '1994'],\n",
       "  ['7', 'Shunichi Kobayashi', '2.10', '1994'],\n",
       "  ['7', 'Stefan Holm', '2.10', '1994'],\n",
       "  ['9', 'Tomohiro Nomura', '2.10', '1994'],\n",
       "  ['10', 'Henry Patterson', '2.10', '1994'],\n",
       "  ['11', 'Bjørn Olsson', '2.10', '1994'],\n",
       "  ['12', 'Stuart Ohrland', '2.05', '1994']]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data = []\n",
    "for file in os.listdir(os.path.join(current_dir, \"html\")):\n",
    "    url = os.path.join(current_dir, \"html\", file)\n",
    "    with open(url) as _f:\n",
    "        url_data = _f.read()\n",
    "    response = scrapy.http.TextResponse(url, body=url_data, encoding='utf-8')\n",
    "    parsed_data.append(process(response))\n",
    "\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interaction with `scrapy` in a Jupyter Notebook is useful because you don't need to run the special shell and you also don't need to run the whole spider. Once you learn what you need to do here, you can adapat the spider to persist data.\n",
    "First, start by persisting data as JSON. To do this, you will need to keep the information in a Python data structure like a dictionary, and then load it as a JSON object, and finally, save it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# You can convert Python into JSON first, but there is no need if you use `json.dump()`\n",
    "# as shown next\n",
    "json_data = json.dumps(parsed_data)\n",
    "\n",
    "# Persist it in a file:\n",
    "with open(\"results.json\", \"w\") as _f:\n",
    "    # use dump() with the Python dictionary directly. \n",
    "    # the conversion is done on the fly\n",
    "    json.dump(parsed_data, _f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can persist the scrapped data as JSON, you can also use CSV. This is specially useful if you want to to some data science operations. Although you can use an advanced library like Pandas for this, you can use the standard library CSV module from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the data first\n",
    "\n",
    "column_names = [\"Place\", \"Athlete\", \"Height\", \"Year\"]\n",
    "rows = []\n",
    "\n",
    "for place, athlete, height, year in parsed_data:\n",
    "    rows.append([place, athlete, height, year])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now persist it to disk\n",
    "import csv\n",
    "\n",
    "with open(\"results.csv\", \"w\") as _f:\n",
    "    writer = csv.writer(_f)\n",
    "\n",
    "    # write the column names\n",
    "    writer.writerow(column_names)\n",
    "\n",
    "    # now write the rows\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can persist data to a database. Unlike the JSON and CSV approach, using a database is much more memory efficient. This is the principal reason why you want to use a database instead of a file on disk. Imagine capturing 10GB of data. This could potentially mean that you need 10GB of available memory to hold onto that data before saving it to disk.\n",
    "By using a database, you can save the data as the data is gathered. \n",
    "\n",
    "For the next cells, use a SQLite database to persist the data. Create the file-based database and the table needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "db_table = 'CREATE TABLE results (id integer primary key, medal TEXT, athlete TEXT)'\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(db_table)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it is time to persist the data. Open the connection again\n",
    "connection = sqlite3.connect(\"1992_results.db\")\n",
    "cursor = connection.cursor()\n",
    "query = 'INSERT INTO results(medal, athlete) VALUES(?, ?)'\n",
    "\n",
    "for tr in table.xpath('tr'):\n",
    "    medal = tr.xpath('td/b/text()').extract()[0]\n",
    "    athlete = tr.xpath('td/a/text()').extract()[0]\n",
    "    cursor.execute(query, (medal, athlete)) \n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now persisted in a file-based database that you can query. Verify that all works by creating a new connection and querying the database.\n",
    "\n",
    "Update the _wikipedia_ project and spider to use some of these techniques to persist data. Next, try parsing all the files in the _html_ directory instead of just one and persist all results. Do you think you can parse other information as well? \n",
    "\n",
    "Try parsing the height and the results from all the other athletes, not just the top three places."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2a18e7aa35240a52af99ab71370101fdc3f0f419f0b3683d6828ff1fe801d36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
